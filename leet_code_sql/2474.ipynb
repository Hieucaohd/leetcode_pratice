{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524583/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/18 16:39:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/18 16:39:53 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders table:\n",
      "   order_id  customer_id order_date  price\n",
      "0         1            1 2019-07-01   1100\n",
      "1         2            1 2019-11-01   1200\n",
      "2         3            1 2020-05-26   3000\n",
      "3         4            1 2021-08-31   3100\n",
      "4         5            1 2022-12-07   4700\n",
      "5         6            2 2015-01-01    700\n",
      "6         7            2 2017-11-07   1000\n",
      "7         8            3 2017-01-01    900\n",
      "8         9            3 2018-11-07    900\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Orders table\n",
    "orders_data = {\n",
    "    'order_id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'customer_id': [1, 1, 1, 1, 1, 2, 2, 3, 3],\n",
    "    'order_date': ['2019-07-01', '2019-11-01', '2020-05-26', '2021-08-31', '2022-12-07',\n",
    "                   '2015-01-01', '2017-11-07', '2017-01-01', '2018-11-07'],\n",
    "    'price': [1100, 1200, 3000, 3100, 4700, 700, 1000, 900, 900]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Orders\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "# Convert order_date to datetime type\n",
    "orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Orders table:\")\n",
    "print(orders_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(orders_df)\n",
    "df_person.createOrReplaceTempView(\"Orders\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with cus_year as (\n",
    "        select\n",
    "            customer_id,\n",
    "            explode(sequence(min_year, max_year)) as year\n",
    "        from (\n",
    "            select\n",
    "                distinct\n",
    "                customer_id,\n",
    "                min_year,\n",
    "                max_year\n",
    "            from (\n",
    "                select\n",
    "                *,\n",
    "                    min(year(order_date)) over (partition by customer_id) as min_year,\n",
    "                    max(year(order_date)) over (partition by customer_id) as max_year\n",
    "                from Orders o \n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    cus_year_price as (\n",
    "        select\n",
    "            customer_id,\n",
    "            year,\n",
    "            sum(price) as price\n",
    "        from (\n",
    "            select\n",
    "                customer_id,\n",
    "                year(order_date) as year,\n",
    "                price\n",
    "            from Orders o \n",
    "        ) group by customer_id, year\n",
    "    ),\n",
    "    \n",
    "    check as (\n",
    "        select\n",
    "            *,\n",
    "            price > lag(price, 1, -1) over(partition by customer_id order by year asc) as is_bigger_last\n",
    "        from (\n",
    "            select\n",
    "                cy.customer_id,\n",
    "                cy.year,\n",
    "                ifnull(cp.price, 0) as price\n",
    "            from cus_year cy left join cus_year_price cp on cy.customer_id = cp.customer_id and cy.year = cp.year \n",
    "            order by cy.customer_id, cy.year\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        distinct \n",
    "        customer_id\n",
    "    from check c where customer_id not in (select customer_id from check where is_bigger_last = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
