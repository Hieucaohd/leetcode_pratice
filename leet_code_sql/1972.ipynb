{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_867817/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/16 17:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/16 17:29:40 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calls table:\n",
      "   caller_id  recipient_id            call_time\n",
      "0          8             4  2021-08-24 17:46:07\n",
      "1          4             8  2021-08-24 19:57:13\n",
      "2          5             1  2021-08-11 05:28:44\n",
      "3          8             3  2021-08-17 04:04:15\n",
      "4         11             3  2021-08-17 13:07:00\n",
      "5          8            11  2021-08-17 22:22:22\n",
      "6          4            12  2021-08-24 22:22:22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the Calls table\n",
    "calls_data = {\n",
    "    'caller_id': [8, 4, 5, 8, 11, 8, 4],\n",
    "    'recipient_id': [4, 8, 1, 3, 3, 11, 12],\n",
    "    'call_time': [\n",
    "        '2021-08-24 17:46:07',\n",
    "        '2021-08-24 19:57:13',\n",
    "        '2021-08-11 05:28:44',\n",
    "        '2021-08-17 04:04:15',\n",
    "        '2021-08-17 13:07:00',\n",
    "        '2021-08-17 22:22:22',\n",
    "        '2021-08-24 22:22:22',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert data to DataFrame\n",
    "calls_df = pd.DataFrame(calls_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Calls table:\")\n",
    "print(calls_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(calls_df)\n",
    "df_person.createOrReplaceTempView(\"Calls\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with t as (\n",
    "        select\n",
    "            user_id,\n",
    "            call_with_id,\n",
    "            call_time,\n",
    "            date(call_time) as date,\n",
    "            call_time = min(call_time) over (partition by user_id, date(call_time)) as is_min,\n",
    "            call_time = max(call_time) over (partition by user_id, date(call_time)) as is_max\n",
    "        from (\n",
    "            select caller_id as user_id, recipient_id as call_with_id, call_time from Calls\n",
    "            union all\n",
    "            select recipient_id as user_id, caller_id as call_with_id, call_time from Calls\n",
    "        ) order by user_id, call_time, call_with_id\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        distinct\n",
    "        t_min.user_id\n",
    "    from (\n",
    "        select user_id, call_with_id, date from t where is_min\n",
    "    ) t_min inner join (\n",
    "        select user_id, call_with_id, date from t where is_max\n",
    "    ) as t_max \n",
    "        on t_min.user_id = t_max.user_id\n",
    "        and t_min.call_with_id = t_max.call_with_id\n",
    "        and t_min.date = t_max.date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      5|\n",
      "|      1|\n",
      "|     12|\n",
      "|      8|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
