{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37289/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/13 10:30:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/13 10:30:10 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary table:\n",
      "   id  employee_id  amount    pay_date\n",
      "0   1            1    9000  2017-03-31\n",
      "1   2            2    6000  2017-03-31\n",
      "2   3            3   10000  2017-03-31\n",
      "3   4            1    7000  2017-02-28\n",
      "4   5            2    6000  2017-02-28\n",
      "5   6            3    8000  2017-02-28\n",
      "\n",
      "Employee table:\n",
      "   employee_id  department_id\n",
      "0            1              1\n",
      "1            2              2\n",
      "2            3              2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the Salary table\n",
    "salary_data = {\n",
    "    'id': [1, 2, 3, 4, 5, 6],\n",
    "    'employee_id': [1, 2, 3, 1, 2, 3],\n",
    "    'amount': [9000, 6000, 10000, 7000, 6000, 8000],\n",
    "    'pay_date': ['2017-03-31', '2017-03-31', '2017-03-31', '2017-02-28', '2017-02-28', '2017-02-28']\n",
    "}\n",
    "\n",
    "# Sample data for the Employee table\n",
    "employee_data = {\n",
    "    'employee_id': [1, 2, 3],\n",
    "    'department_id': [1, 2, 2]\n",
    "}\n",
    "\n",
    "# Creating pandas DataFrames for Salary and Employee tables\n",
    "salary_df = pd.DataFrame(salary_data)\n",
    "employee_df = pd.DataFrame(employee_data)\n",
    "\n",
    "# Displaying the Salary and Employee DataFrames\n",
    "print(\"Salary table:\")\n",
    "print(salary_df)\n",
    "print(\"\\nEmployee table:\")\n",
    "print(employee_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(salary_df)\n",
    "df_person.createOrReplaceTempView(\"Salary\")\n",
    "\n",
    "df_person = spark.createDataFrame(employee_df)\n",
    "df_person.createOrReplaceTempView(\"Employee\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select month as pay_month, department_id, comparison from (select\n",
    "        distinct month, department_id, deparment_salary_per_month, company_salary_per_month,\n",
    "        case\n",
    "            when deparment_salary_per_month = company_salary_per_month then 'same'\n",
    "            when deparment_salary_per_month > company_salary_per_month then 'higher'\n",
    "            else 'lower'\n",
    "        end as comparison \n",
    "    from (\n",
    "        select\n",
    "            s.employee_id,\n",
    "            s.amount,\n",
    "            s.pay_date,\n",
    "            e.department_id,\n",
    "            avg(amount) over (partition by year(s.pay_date), month(s.pay_date)) as company_salary_per_month,\n",
    "            avg(amount) over (partition by year(s.pay_date), month(s.pay_date), e.department_id) as deparment_salary_per_month,\n",
    "            CONCAT(YEAR(pay_date), '-', LPAD(MONTH(pay_date), 2, '0')) as month\n",
    "        from Salary s left join Employee e on s.employee_id = e.employee_id\n",
    "        order by s.pay_date, e.department_id\n",
    "    )) order by department_id, pay_month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+\n",
      "|pay_month|department_id|comparison|\n",
      "+---------+-------------+----------+\n",
      "|  2017-02|            1|      same|\n",
      "|  2017-03|            1|    higher|\n",
      "|  2017-02|            2|      same|\n",
      "|  2017-03|            2|     lower|\n",
      "+---------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
