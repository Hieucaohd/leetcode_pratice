{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309982/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 14:37:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/14 14:37:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   player_id  group_id\n",
      "0         15         1\n",
      "1         25         1\n",
      "2         30         1\n",
      "3         45         1\n",
      "4         10         2\n",
      "5         35         2\n",
      "6         50         2\n",
      "7         20         3\n",
      "8         40         3\n",
      "\n",
      "   match_id  first_player  second_player  first_score  second_score\n",
      "0         1            15             45            3             0\n",
      "1         2            30             25            1             2\n",
      "2         3            30             15            2             0\n",
      "3         4            40             20            5             2\n",
      "4         5            35             50            1             1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Players table\n",
    "players_data = {\n",
    "    'player_id': [15, 25, 30, 45, 10, 35, 50, 20, 40],\n",
    "    'group_id': [1, 1, 1, 1, 2, 2, 2, 3, 3]\n",
    "}\n",
    "\n",
    "# Sample data for Matches table\n",
    "matches_data = {\n",
    "    'match_id': [1, 2, 3, 4, 5],\n",
    "    'first_player': [15, 30, 30, 40, 35],\n",
    "    'second_player': [45, 25, 15, 20, 50],\n",
    "    'first_score': [3, 1, 2, 5, 1],\n",
    "    'second_score': [0, 2, 0, 2, 1]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Players and Matches\n",
    "players_df = pd.DataFrame(players_data)\n",
    "matches_df = pd.DataFrame(matches_data)\n",
    "\n",
    "\n",
    "\n",
    "print(players_df)\n",
    "print()\n",
    "print(matches_df)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(players_df)\n",
    "df_person.createOrReplaceTempView(\"Players\")\n",
    "\n",
    "df_person = spark.createDataFrame(matches_df)\n",
    "df_person.createOrReplaceTempView(\"Matches\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with first_players as (\n",
    "        select\n",
    "            first_player as player_id,\n",
    "            first_score as score\n",
    "        from Matches m\n",
    "    ),\n",
    "    \n",
    "    second_players as (\n",
    "        select\n",
    "            second_player as player_id,\n",
    "            second_score as score\n",
    "        from Matches m\n",
    "    ),\n",
    "    \n",
    "    all_players as (\n",
    "        select\n",
    "            t.player_id,\n",
    "            sum(t.score) as score,\n",
    "            p.group_id\n",
    "        from (\n",
    "            select * from first_players\n",
    "            union all\n",
    "            select * from second_players\n",
    "        ) t left join Players p on t.player_id = p.player_id\n",
    "        group by t.player_id, p.group_id\n",
    "    )\n",
    "    \n",
    "    select \n",
    "        group_id,\n",
    "        player_id\n",
    "    from (\n",
    "        select\n",
    "            player_id,\n",
    "            score,\n",
    "            group_id,\n",
    "            dense_rank() over (partition by group_id order by score desc, player_id asc) as rk\n",
    "        from all_players a\n",
    "    ) where rk = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|group_id|player_id|\n",
      "+--------+---------+\n",
      "|       1|       15|\n",
      "|       2|       35|\n",
      "|       3|       40|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
