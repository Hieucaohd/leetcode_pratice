{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 06:12:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/14 06:12:40 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name continent\n",
      "0    Jane   America\n",
      "1  Pascal    Europe\n",
      "2      Xi      Asia\n",
      "3    Jack   America\n",
      "\n",
      "continent America Asia  Europe\n",
      "0            Jack  NaN     NaN\n",
      "1            Jane  NaN     NaN\n",
      "2             NaN  NaN     NaN\n",
      "3             NaN   Xi  Pascal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the Student table\n",
    "student_data = {\n",
    "    'name': ['Jane', 'Pascal', 'Xi', 'Jack'],\n",
    "    'continent': ['America', 'Europe', 'Asia', 'America']\n",
    "}\n",
    "\n",
    "# Creating a pandas DataFrame for the Student table\n",
    "student_df = pd.DataFrame(student_data)\n",
    "\n",
    "# Pivot the continent column\n",
    "pivot_df = student_df.pivot(columns='continent', values='name')\n",
    "\n",
    "# Reorder the columns\n",
    "pivot_df = pivot_df[['America', 'Asia', 'Europe']]\n",
    "\n",
    "# Sort the values alphabetically within each column\n",
    "pivot_df = pivot_df.apply(lambda x: sorted(x, key=lambda v: str(v).lower()))\n",
    "\n",
    "# Display the result\n",
    "print(student_df)\n",
    "print()\n",
    "print(pivot_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(student_df)\n",
    "df_person.createOrReplaceTempView(\"Student\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select\n",
    "        *,\n",
    "        row_number() over (partition by continent order by name asc) as rk\n",
    "    from Student s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "    select * from (\n",
    "        select\n",
    "            *,\n",
    "            row_number() over (partition by continent order by name asc) as rk\n",
    "        from Student s\n",
    "    )\n",
    "    pivot (\n",
    "        max(name)\n",
    "        for continent in (\n",
    "            'America', 'Asia', 'Europe'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+\n",
      "|  name|continent| rk|\n",
      "+------+---------+---+\n",
      "|  Jack|  America|  1|\n",
      "|  Jane|  America|  2|\n",
      "|    Xi|     Asia|  1|\n",
      "|Pascal|   Europe|  1|\n",
      "+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| rk|America|Asia|Europe|\n",
      "+---+-------+----+------+\n",
      "|  1|   Jack|  Xi|Pascal|\n",
      "|  2|   Jane|NULL|  NULL|\n",
      "+---+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
