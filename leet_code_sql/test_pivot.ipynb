{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_308104/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 11:01:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/15 11:01:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/15 11:01:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  temp\n",
      "0  2016-01-01    72\n",
      "1  2016-01-02    96\n",
      "2  2016-01-03    73\n",
      "3  2016-01-04    77\n",
      "4  2016-01-05    77\n",
      "..        ...   ...\n",
      "26 2019-12-27    85\n",
      "27 2019-12-28    83\n",
      "28 2019-12-29    83\n",
      "29 2019-12-30    98\n",
      "30 2019-12-31    86\n",
      "\n",
      "[1461 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Function to generate random temperature data for a given month and year\n",
    "def generate_temperature_data(year, month):\n",
    "    start_date = datetime(year, month, 1)\n",
    "    end_date = start_date + timedelta(days=pd.Period(start_date, 'M').days_in_month - 1)\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "    temps = [random.randint(70, 100) for _ in range(len(dates))]\n",
    "    return pd.DataFrame({'date': dates, 'temp': temps})\n",
    "\n",
    "# Generate temperature data for each month from January to December for the years 2016 to 2019\n",
    "data = pd.concat([generate_temperature_data(year, month) for year in range(2016, 2020) for month in range(1, 13)])\n",
    "\n",
    "# Display the generated data\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(data)\n",
    "df_person.createOrReplaceTempView(\"high_temps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT year(date) year, month(date) month, temp\n",
    "  FROM high_temps\n",
    "  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|year|month|temp|\n",
      "+----+-----+----+\n",
      "|2016|    1|  91|\n",
      "|2016|    1|  73|\n",
      "|2016|    1|  99|\n",
      "|2016|    1|  73|\n",
      "|2016|    1|  83|\n",
      "|2016|    1|  70|\n",
      "|2016|    1|  89|\n",
      "|2016|    1|  80|\n",
      "|2016|    1|  82|\n",
      "|2016|    1|  75|\n",
      "|2016|    1|  96|\n",
      "|2016|    1|  83|\n",
      "|2016|    1|  92|\n",
      "|2016|    1|  87|\n",
      "|2016|    1|  70|\n",
      "|2016|    1|  74|\n",
      "|2016|    1|  75|\n",
      "|2016|    1|  85|\n",
      "|2016|    1|  71|\n",
      "|2016|    1|  81|\n",
      "+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "    SELECT * FROM (\n",
    "  SELECT year(date) year, month(date) month, temp\n",
    "  FROM high_temps\n",
    "  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'\n",
    ")\n",
    "PIVOT (\n",
    "  CAST(avg(temp) AS DECIMAL(4, 1))\n",
    "  FOR month in (\n",
    "    1 JAN, 2 FEB, 3 MAR, 4 APR, 5 MAY, 6 JUN,\n",
    "    7 JUL, 8 AUG, 9 SEP, 10 OCT, 11 NOV, 12 DEC\n",
    "  )\n",
    ")\n",
    "ORDER BY year DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|year| JAN| FEB| MAR| APR| MAY| JUN| JUL| AUG| SEP| OCT| NOV| DEC|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2018|82.5|83.1|85.5|88.8|84.1|83.0|86.4|82.8|NULL|NULL|NULL|NULL|\n",
      "|2016|82.4|86.1|87.4|83.2|83.2|83.9|85.0|85.4|85.9|85.2|87.1|84.5|\n",
      "|2017|82.3|85.8|85.6|83.4|84.4|87.0|87.3|86.2|85.4|83.6|86.1|85.1|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "        SELECT * FROM (\n",
    "  SELECT year(date) year, month(date) month, temp\n",
    "  FROM high_temps\n",
    "  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|year|month|temp|\n",
      "+----+-----+----+\n",
      "|2016|    1|  72|\n",
      "|2016|    1|  96|\n",
      "|2016|    1|  73|\n",
      "|2016|    1|  77|\n",
      "|2016|    1|  77|\n",
      "|2016|    1|  82|\n",
      "|2016|    1|  79|\n",
      "|2016|    1| 100|\n",
      "|2016|    1|  85|\n",
      "|2016|    1|  90|\n",
      "|2016|    1|  84|\n",
      "|2016|    1|  98|\n",
      "|2016|    1|  87|\n",
      "|2016|    1|  96|\n",
      "|2016|    1|  70|\n",
      "|2016|    1|  90|\n",
      "|2016|    1|  85|\n",
      "|2016|    1|  81|\n",
      "|2016|    1|  99|\n",
      "|2016|    1|  92|\n",
      "+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_2).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
