{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_334707/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 15:17:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/14 15:17:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Failed table\n",
    "failed_data = {\n",
    "    'fail_date': ['2018-12-28', '2018-12-29', '2019-01-04', '2019-01-05']\n",
    "}\n",
    "\n",
    "# Sample data for Succeeded table\n",
    "succeeded_data = {\n",
    "    'success_date': ['2018-12-30', '2018-12-31', '2019-01-01', '2019-01-02', '2019-01-03', '2019-01-06']\n",
    "}\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "failed_df = pd.DataFrame(failed_data)\n",
    "failed_df['fail_date'] = pd.to_datetime(failed_df['fail_date'])\n",
    "\n",
    "succeeded_df = pd.DataFrame(succeeded_data)\n",
    "succeeded_df['success_date'] = pd.to_datetime(succeeded_df['success_date'])\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(failed_df)\n",
    "df_person.createOrReplaceTempView(\"Failed\")\n",
    "\n",
    "df_person = spark.createDataFrame(succeeded_df)\n",
    "df_person.createOrReplaceTempView(\"Succeeded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fail_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fail_date\n",
       "0 2018-12-28\n",
       "1 2018-12-29\n",
       "2 2019-01-04\n",
       "3 2019-01-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  success_date\n",
       "0   2018-12-30\n",
       "1   2018-12-31\n",
       "2   2019-01-01\n",
       "3   2019-01-02\n",
       "4   2019-01-03\n",
       "5   2019-01-06"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "succeeded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with all_day as (\n",
    "        select\n",
    "            *\n",
    "        from (\n",
    "            select fail_date as date_, 'failed' as state from Failed\n",
    "            union all\n",
    "            select success_date as date_, 'success'as state from Succeeded \n",
    "        )\n",
    "        where date_ >= \"2019-01-01\" and date_ <= \"2019-12-31\"\n",
    "        order by date_ asc\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        state as period_state, \n",
    "        min(date_) as start_date,\n",
    "        max(date_) as end_date\n",
    "    from (\n",
    "        select\n",
    "            *,\n",
    "            sum(\n",
    "                mark\n",
    "            ) over (order by date_ asc) as group_id\n",
    "        from (\n",
    "            select \n",
    "                *,\n",
    "                case\n",
    "                    when lag(state, 1, state) over (order by date_ asc) = state then 0\n",
    "                    else 1\n",
    "                end as mark\n",
    "            from all_day a\n",
    "        )\n",
    "    ) group by group_id, state\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 15:37:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/14 15:37:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+\n",
      "|period_state|         start_date|           end_date|\n",
      "+------------+-------------------+-------------------+\n",
      "|     success|2019-01-01 00:00:00|2019-01-03 00:00:00|\n",
      "|      failed|2019-01-04 00:00:00|2019-01-05 00:00:00|\n",
      "|     success|2019-01-06 00:00:00|2019-01-06 00:00:00|\n",
      "+------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "    with all_day as (\n",
    "        select\n",
    "            *\n",
    "        from (\n",
    "            select fail_date as date_, 'failed' as state from Failed\n",
    "            union all\n",
    "            select success_date as date_, 'success'as state from Succeeded \n",
    "        )\n",
    "        where date_ >= \"2019-01-01\" and date_ <= \"2019-12-31\"\n",
    "        order by date_ asc\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "        select \n",
    "            *,\n",
    "            date_sub(\n",
    "                date_,\n",
    "                rank() over (\n",
    "                    partition by state\n",
    "                    order by date_\n",
    "                ) \n",
    "            ) as pt\n",
    "        from all_day a order by date_ asc\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+\n",
      "|              date_|  state|        pt|\n",
      "+-------------------+-------+----------+\n",
      "|2019-01-01 00:00:00|success|2018-12-31|\n",
      "|2019-01-02 00:00:00|success|2018-12-31|\n",
      "|2019-01-03 00:00:00|success|2018-12-31|\n",
      "|2019-01-04 00:00:00| failed|2019-01-03|\n",
      "|2019-01-05 00:00:00| failed|2019-01-03|\n",
      "|2019-01-06 00:00:00|success|2019-01-02|\n",
      "+-------------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
