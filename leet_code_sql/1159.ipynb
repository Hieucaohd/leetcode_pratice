{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4006/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/13 09:37:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/13 09:37:24 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users table:\n",
      "   user_id   join_date favorite_brand\n",
      "0        1  2019-01-01         Lenovo\n",
      "1        2  2019-02-09        Samsung\n",
      "2        3  2019-01-19             LG\n",
      "3        4  2019-05-21             HP\n",
      "\n",
      "Orders table:\n",
      "   order_id  order_date  item_id  buyer_id  seller_id\n",
      "0         1  2019-08-01        4         1          2\n",
      "1         2  2019-08-02        2         1          3\n",
      "2         3  2019-08-03        3         2          3\n",
      "3         4  2019-08-04        1         4          2\n",
      "4         5  2019-08-04        1         3          4\n",
      "5         6  2019-08-05        2         2          4\n",
      "\n",
      "Items table:\n",
      "   item_id item_brand\n",
      "0        1    Samsung\n",
      "1        2     Lenovo\n",
      "2        3         LG\n",
      "3        4         HP\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the Users table\n",
    "users_data = {\n",
    "    'user_id': [1, 2, 3, 4],\n",
    "    'join_date': ['2019-01-01', '2019-02-09', '2019-01-19', '2019-05-21'],\n",
    "    'favorite_brand': ['Lenovo', 'Samsung', 'LG', 'HP']\n",
    "}\n",
    "\n",
    "# Sample data for the Orders table\n",
    "orders_data = {\n",
    "    'order_id': [1, 2, 3, 4, 5, 6],\n",
    "    'order_date': ['2019-08-01', '2019-08-02', '2019-08-03', '2019-08-04', '2019-08-04', '2019-08-05'],\n",
    "    'item_id': [4, 2, 3, 1, 1, 2],\n",
    "    'buyer_id': [1, 1, 2, 4, 3, 2],\n",
    "    'seller_id': [2, 3, 3, 2, 4, 4]\n",
    "}\n",
    "\n",
    "# Sample data for the Items table\n",
    "items_data = {\n",
    "    'item_id': [1, 2, 3, 4],\n",
    "    'item_brand': ['Samsung', 'Lenovo', 'LG', 'HP']\n",
    "}\n",
    "\n",
    "# Creating pandas DataFrames for Users, Orders, and Items tables\n",
    "users_df = pd.DataFrame(users_data)\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "items_df = pd.DataFrame(items_data)\n",
    "\n",
    "# Displaying the Users, Orders, and Items DataFrames\n",
    "print(\"Users table:\")\n",
    "print(users_df)\n",
    "print(\"\\nOrders table:\")\n",
    "print(orders_df)\n",
    "print(\"\\nItems table:\")\n",
    "print(items_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(users_df)\n",
    "df_person.createOrReplaceTempView(\"Users\")\n",
    "\n",
    "df_person = spark.createDataFrame(orders_df)\n",
    "df_person.createOrReplaceTempView(\"Orders\")\n",
    "\n",
    "df_person = spark.createDataFrame(items_df)\n",
    "df_person.createOrReplaceTempView(\"Items\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with t as (\n",
    "        select\n",
    "            *,\n",
    "            case\n",
    "                when rank_sell = 2 and favorite_brand = item_brand then 'yes'\n",
    "                else 'no'\n",
    "            end 2nd_item_fav_brand \n",
    "        from (\n",
    "            select\n",
    "                o.seller_id,\n",
    "                o.order_date,\n",
    "                u.favorite_brand,\n",
    "                i.item_brand,\n",
    "                row_number() over (partition by o.seller_id order by order_date asc) as rank_sell\n",
    "            from\n",
    "                Orders o\n",
    "                left join Users u on o.seller_id = u.user_id\n",
    "                left join Items i on o.item_id = i.item_id\n",
    "            order by o.seller_id asc, o.order_date asc\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    k as (select distinct seller_id, 2nd_item_fav_brand from t where 2nd_item_fav_brand = 'yes')\n",
    "\n",
    "    select distinct seller_id, 2nd_item_fav_brand from t where 2nd_item_fav_brand != 'yes' and seller_id not in (select seller_id from k) union all select * from k\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "    with o as (select\n",
    "        o.seller_id,\n",
    "        o.order_date,\n",
    "        o.item_id,\n",
    "        row_number() over (partition by o.seller_id order by o.order_date asc) as rk\n",
    "    from Orders o)\n",
    "    \n",
    "    select\n",
    "        u.user_id,\n",
    "        case\n",
    "            when u.favorite_brand = i.item_brand then 'yes'\n",
    "            else 'no'\n",
    "        end as 2nd_item_fav_brand   \n",
    "    from Users u\n",
    "    left join o on u.user_id = o.seller_id and o.rk = 2\n",
    "    left join Items i on o.item_id = i.item_id\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|2nd_item_fav_brand|\n",
      "+-------+------------------+\n",
      "|      1|                no|\n",
      "|      2|               yes|\n",
      "|      3|               yes|\n",
      "|      4|                no|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
