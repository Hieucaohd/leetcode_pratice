{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42277/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/21 03:51:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/21 03:51:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/21 03:51:53 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions table:\n",
      "   user_id        session_start          session_end  session_id session_type\n",
      "0      101  2023-11-06 13:53:42  2023-11-06 14:05:42         375       Viewer\n",
      "1      101  2023-11-22 16:45:21  2023-11-22 20:39:21         594     Streamer\n",
      "2      102  2023-11-16 13:23:09  2023-11-16 16:10:09         777     Streamer\n",
      "3      102  2023-11-17 13:23:09  2023-11-17 16:10:09         778     Streamer\n",
      "4      101  2023-11-20 07:16:06  2023-11-20 08:33:06         315     Streamer\n",
      "5      104  2023-11-27 03:10:49  2023-11-27 03:30:49         797       Viewer\n",
      "6      103  2023-11-27 03:10:49  2023-11-27 03:30:49         798     Streamer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Sessions table\n",
    "sessions_data = {\n",
    "    'user_id': [101, 101, 102, 102, 101, 104, 103],\n",
    "    'session_start': ['2023-11-06 13:53:42', '2023-11-22 16:45:21', '2023-11-16 13:23:09', '2023-11-17 13:23:09', '2023-11-20 07:16:06', '2023-11-27 03:10:49', '2023-11-27 03:10:49'],\n",
    "    'session_end': ['2023-11-06 14:05:42', '2023-11-22 20:39:21', '2023-11-16 16:10:09', '2023-11-17 16:10:09', '2023-11-20 08:33:06', '2023-11-27 03:30:49', '2023-11-27 03:30:49'],\n",
    "    'session_id': [375, 594, 777, 778, 315, 797, 798],\n",
    "    'session_type': ['Viewer', 'Streamer', 'Streamer', 'Streamer', 'Streamer', 'Viewer', 'Streamer']\n",
    "}\n",
    "\n",
    "# Create DataFrame for Sessions\n",
    "sessions_df = pd.DataFrame(sessions_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Sessions table:\")\n",
    "print(sessions_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(sessions_df)\n",
    "df_person.createOrReplaceTempView(\"Sessions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with get_rank as (\n",
    "        select\n",
    "            *,\n",
    "            row_number() over(partition by user_id order by session_start asc) as rk,\n",
    "            count(*) over(partition by user_id) as count_session\n",
    "        from Sessions s \n",
    "    )\n",
    "    \n",
    "    select \n",
    "        distinct \n",
    "        u.user_id,\n",
    "        count(*) as session_count\n",
    "    from (\n",
    "        select\n",
    "            distinct user_id\n",
    "        from get_rank where rk = 1 and session_type = 'Viewer' and count_session >= 2\n",
    "    ) u left join Sessions s on u.user_id = s.user_id\n",
    "    where s.session_type = 'Streamer'\n",
    "    group by u.user_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|user_id|session_count|\n",
      "+-------+-------------+\n",
      "|    101|            2|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
