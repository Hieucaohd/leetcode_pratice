{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_768/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/22 16:01:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/22 16:01:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heights table:\n",
      "    id  height\n",
      "0    1       0\n",
      "1    2       1\n",
      "2    3       0\n",
      "3    4       2\n",
      "4    5       1\n",
      "5    6       0\n",
      "6    7       1\n",
      "7    8       3\n",
      "8    9       2\n",
      "9   10       1\n",
      "10  11       2\n",
      "11  12       1\n",
      "Heights_1 table:\n",
      "   id  height\n",
      "0   1       0\n",
      "1   2       1\n",
      "2   3       2\n",
      "3   4       3\n",
      "4   5       0\n",
      "5   6       1\n",
      "6   7       0\n",
      "7   8       5\n",
      "Heights_1 table:\n",
      "    id  height\n",
      "0    1       0\n",
      "1    2       1\n",
      "2    3       0\n",
      "3    4       2\n",
      "4    5       1\n",
      "5    6       0\n",
      "6    7       1\n",
      "7    8       2\n",
      "8    9       2\n",
      "9   10       2\n",
      "10  11       2\n",
      "11  12       2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Heights table\n",
    "heights_data = {\n",
    "    'id': list(range(1, 13)),\n",
    "    'height': [0, 1, 0, 2, 1, 0, 1, 3, 2, 1, 2, 1]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Heights\n",
    "heights_df = pd.DataFrame(heights_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Heights table:\")\n",
    "print(heights_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(heights_df)\n",
    "df_person.createOrReplaceTempView(\"Heights\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for Heights table\n",
    "heights_data = {\n",
    "    'id':     [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'height': [0, 1, 2, 3, 0, 1, 0, 5]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Heights\n",
    "heights_df = pd.DataFrame(heights_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Heights_1 table:\")\n",
    "print(heights_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(heights_df)\n",
    "df_person.createOrReplaceTempView(\"Heights_1\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for Heights table\n",
    "heights_data = {\n",
    "    'id':     list(range(1, 13)),\n",
    "    'height': [0, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 2]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Heights\n",
    "heights_df = pd.DataFrame(heights_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Heights_1 table:\")\n",
    "print(heights_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(heights_df)\n",
    "df_person.createOrReplaceTempView(\"Heights_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"Heights_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = f\"\"\"\n",
    "    with bound_height as (\n",
    "        select\n",
    "            h.id as start_id,\n",
    "            h.height as start_height,\n",
    "            h1.id as end_id,\n",
    "            h1.height as end_height\n",
    "        from {table_name} as h left join {table_name} h1 on h.id < h1.id - 1\n",
    "        where h1.id is not null\n",
    "        order by h.id\n",
    "    ),\n",
    "    \n",
    "    range_height as (\n",
    "        select\n",
    "            b.*,\n",
    "            h.height as between_height,\n",
    "            (h.height < b.start_height and h.height < b.end_height) as is_less_than_bound\n",
    "        from bound_height as b left join {table_name} as h on h.id > b.start_id and h.id < b.end_id\n",
    "    ),\n",
    "    \n",
    "    bound as (\n",
    "        select\n",
    "            *\n",
    "        from bound_height as b where (b.start_id, b.end_id) not in (\n",
    "            select start_id, end_id from range_height where  not is_less_than_bound\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    satisfy_bound as (\n",
    "        select\n",
    "            s.*\n",
    "        from bound s left join bound s1 \n",
    "            on  s.start_id >= s1.start_id\n",
    "            and s.end_id <= s1.end_id\n",
    "            and pow(s.start_id - s1.start_id, 2) + pow(s.end_id - s1.end_id, 2) != 0\n",
    "        where s1.start_id is null\n",
    "    )\n",
    "    \n",
    "    select \n",
    "        sum(remain_height) total_trapped_water \n",
    "    from (\n",
    "        select\n",
    "            s.*,\n",
    "            h.*,\n",
    "            least(s.start_height, s.end_height) - h.height as remain_height\n",
    "        from satisfy_bound s left join {table_name} h \n",
    "            on h.id > s.start_id and h.id < s.end_id\n",
    "    )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|total_trapped_water|\n",
      "+-------------------+\n",
      "|                  5|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show(700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
