{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15654/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 16:46:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/19 16:46:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions table:\n",
      "                customer_id transaction_date  amount\n",
      "transaction_id                                      \n",
      "1                       101       2023-05-01     100\n",
      "2                       101       2023-05-02     150\n",
      "3                       101       2023-05-03     200\n",
      "4                       102       2023-05-01      50\n",
      "5                       102       2023-05-03     100\n",
      "6                       102       2023-05-04     200\n",
      "7                       105       2023-05-01     100\n",
      "8                       105       2023-05-02     150\n",
      "9                       105       2023-05-03     200\n",
      "10                      105       2023-05-04     300\n",
      "11                      105       2023-05-12     250\n",
      "12                      105       2023-05-13     260\n",
      "13                      105       2023-05-14     270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Transactions table\n",
    "transactions_data = {\n",
    "    'transaction_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "    'customer_id': [101, 101, 101, 102, 102, 102, 105, 105, 105, 105, 105, 105, 105],\n",
    "    'transaction_date': ['2023-05-01', '2023-05-02', '2023-05-03', '2023-05-01', '2023-05-03', '2023-05-04', \n",
    "                         '2023-05-01', '2023-05-02', '2023-05-03', '2023-05-04', '2023-05-12', '2023-05-13', '2023-05-14'],\n",
    "    'amount': [100, 150, 200, 50, 100, 200, 100, 150, 200, 300, 250, 260, 270]\n",
    "}\n",
    "\n",
    "# Create DataFrame for Transactions\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "\n",
    "# Convert date column to datetime type\n",
    "transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])\n",
    "\n",
    "# Set transaction_id as the primary key\n",
    "transactions_df.set_index('transaction_id', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Transactions table:\")\n",
    "print(transactions_df)\n",
    "\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(transactions_df)\n",
    "df_person.createOrReplaceTempView(\"Transactions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with cus_and_day as (\n",
    "        select\n",
    "            customer_id,\n",
    "            date_add(min_transaction_date, day_add) as day\n",
    "        from (\n",
    "            select\n",
    "                customer_id,\n",
    "                min(transaction_date) as min_transaction_date,\n",
    "                max(transaction_date) as max_transaction_date,\n",
    "                explode(sequence(0, date_diff(max(transaction_date), min(transaction_date)))) as day_add\n",
    "            from Transactions t\n",
    "            group by customer_id\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            c.customer_id,\n",
    "            c.day,\n",
    "            ifnull(t.amount, 0) as amount\n",
    "        from cus_and_day c left join Transactions t on c.customer_id = t.customer_id and c.day = t.transaction_date\n",
    "        order by c.customer_id, c.day\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|customer_id|       day|amount|\n",
      "+-----------+----------+------+\n",
      "|        101|2023-05-01|   100|\n",
      "|        101|2023-05-02|   150|\n",
      "|        101|2023-05-03|   200|\n",
      "|        102|2023-05-01|    50|\n",
      "|        102|2023-05-02|     0|\n",
      "|        102|2023-05-03|   100|\n",
      "|        102|2023-05-04|   200|\n",
      "|        105|2023-05-01|   100|\n",
      "|        105|2023-05-02|   150|\n",
      "|        105|2023-05-03|   200|\n",
      "|        105|2023-05-04|   300|\n",
      "|        105|2023-05-05|     0|\n",
      "|        105|2023-05-06|     0|\n",
      "|        105|2023-05-07|     0|\n",
      "|        105|2023-05-08|     0|\n",
      "|        105|2023-05-09|     0|\n",
      "|        105|2023-05-10|     0|\n",
      "|        105|2023-05-11|     0|\n",
      "|        105|2023-05-12|   250|\n",
      "|        105|2023-05-13|   260|\n",
      "|        105|2023-05-14|   270|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "    select\n",
    "        customer_id,\n",
    "        consecutive_start,\n",
    "        consecutive_end\n",
    "    from (\n",
    "        select\n",
    "            customer_id,\n",
    "            min(transaction_date) as consecutive_start,\n",
    "            max(transaction_date) as consecutive_end,\n",
    "            count(*) as number_in_group\n",
    "        from (\n",
    "            select\n",
    "                *,\n",
    "                sum(\n",
    "                    if(is_consecutive, 0, 1)\n",
    "                ) over(partition by customer_id order by transaction_date asc) as group_id\n",
    "            from (\n",
    "                select\n",
    "                    *,\n",
    "                    (is_previous_day and is_increase) as is_consecutive\n",
    "                from (\n",
    "                    select\n",
    "                        customer_id,\n",
    "                        transaction_date,\n",
    "                        amount,\n",
    "                        lag(transaction_date, 1, transaction_date) over(partition by customer_id order by transaction_date asc) as previous_transaction_date,\n",
    "                        lag(amount, 1, amount) over(partition by customer_id order by transaction_date asc) as previous_amount,\n",
    "                        date_diff(transaction_date, lag(transaction_date, 1, transaction_date) over(partition by customer_id order by transaction_date asc)) = 1 as is_previous_day,\n",
    "                        amount > lag(amount, 1, amount) over(partition by customer_id order by transaction_date asc) as is_increase\n",
    "                    from Transactions t\n",
    "                )\n",
    "            )\n",
    "        ) group by customer_id, group_id\n",
    "    ) where number_in_group >= 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+\n",
      "|customer_id|  consecutive_start|    consecutive_end|\n",
      "+-----------+-------------------+-------------------+\n",
      "|        101|2023-05-01 00:00:00|2023-05-03 00:00:00|\n",
      "|        105|2023-05-01 00:00:00|2023-05-04 00:00:00|\n",
      "|        105|2023-05-12 00:00:00|2023-05-14 00:00:00|\n",
      "+-----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
