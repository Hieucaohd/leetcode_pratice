{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210714/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 11:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/14 11:53:54 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  spend_date platform  amount\n",
      "0        1  2019-07-01   mobile     100\n",
      "1        1  2019-07-01  desktop     100\n",
      "2        2  2019-07-01   mobile     100\n",
      "3        2  2019-07-02   mobile     100\n",
      "4        3  2019-07-01  desktop     100\n",
      "5        3  2019-07-02  desktop     100\n",
      "6        4  2019-07-01   mobile     200\n",
      "7        4  2019-07-01  desktop     300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'user_id': [1, 1, 2, 2, 3, 3, 4, 4],\n",
    "    'spend_date': ['2019-07-01', '2019-07-01', '2019-07-01', '2019-07-02', '2019-07-01', '2019-07-02', '2019-07-01', '2019-07-01'],\n",
    "    'platform': ['mobile', 'desktop', 'mobile', 'mobile', 'desktop', 'desktop', 'mobile', 'desktop'],\n",
    "    'amount': [100, 100, 100, 100, 100, 100, 200, 300]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df_person = spark.createDataFrame(df)\n",
    "df_person.createOrReplaceTempView(\"Spending\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_0 = \"\"\"\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    with t as (\n",
    "        select\n",
    "            spend_date,\n",
    "            platform,\n",
    "            sum(total_amount) as total_amount,\n",
    "            count(distinct user_id) as total_users\n",
    "        from (\n",
    "            select\n",
    "                distinct\n",
    "                user_id,\n",
    "                spend_date,\n",
    "                platform_count as platform,\n",
    "                total_amount\n",
    "            from (\n",
    "                select\n",
    "                    *,\n",
    "                    case \n",
    "                        when count(platform) over (partition by spend_date, user_id) = 2 then 'both' \n",
    "                        else platform\n",
    "                    end as platform_count,\n",
    "                    sum(amount) over (partition by spend_date, user_id) as total_amount\n",
    "                from Spending s \n",
    "            ) \n",
    "        ) group by spend_date, platform\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        k.spend_date,\n",
    "        e.platform,\n",
    "        ifnull(t.total_amount, 0),\n",
    "        ifnull(t.total_users, 0)\n",
    "    from (\n",
    "        select distinct spend_date from t\n",
    "    ) k cross join (\n",
    "        select \"both\" as platform\n",
    "        union\n",
    "        select \"mobile\" as platform\n",
    "        union\n",
    "        select \"desktop\" as platform\n",
    "    ) e left join t on k.spend_date = t.spend_date and e.platform = t.platform\n",
    "    order by k.spend_date, e.platform\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------------------+----------------------+\n",
      "|spend_date|platform|ifnull(total_amount, 0)|ifnull(total_users, 0)|\n",
      "+----------+--------+-----------------------+----------------------+\n",
      "|2019-07-01|    both|                    700|                     2|\n",
      "|2019-07-01| desktop|                    100|                     1|\n",
      "|2019-07-01|  mobile|                    100|                     1|\n",
      "|2019-07-02|    both|                      0|                     0|\n",
      "|2019-07-02| desktop|                    100|                     1|\n",
      "|2019-07-02|  mobile|                    100|                     1|\n",
      "+----------+--------+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"\"\"\n",
    "    select \"both\" as platform\n",
    "    union\n",
    "    select \"mobile\" as platform\n",
    "    union\n",
    "    select \"desktop\" as platform\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|platform|\n",
      "+--------+\n",
      "|    both|\n",
      "|       m|\n",
      "|       d|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(test_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "\n",
    "        select\n",
    "            distinct\n",
    "            user_id,\n",
    "            spend_date,\n",
    "            platform_count as platform,\n",
    "            total_amount\n",
    "        from (\n",
    "            select\n",
    "                *,\n",
    "                case \n",
    "                    when count(platform) over (partition by spend_date, user_id) = 2 then 'both' \n",
    "                    else platform\n",
    "                end as platform_count,\n",
    "                sum(amount) over (partition by spend_date, user_id) as total_amount\n",
    "            from Spending s \n",
    "        ) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+------------+\n",
      "|user_id|spend_date|platform|total_amount|\n",
      "+-------+----------+--------+------------+\n",
      "|      1|2019-07-01|    both|         200|\n",
      "|      2|2019-07-01|  mobile|         100|\n",
      "|      3|2019-07-01| desktop|         100|\n",
      "|      4|2019-07-01|    both|         500|\n",
      "|      2|2019-07-02|  mobile|         100|\n",
      "|      3|2019-07-02| desktop|         100|\n",
      "+-------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
