{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2774/2081191832.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 02:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/15 02:33:38 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for Product table\n",
    "product_data = {\n",
    "    'product_id': [1, 2, 3],\n",
    "    'product_name': ['LC Phone', 'LC T-Shirt', 'LC Keychain']\n",
    "}\n",
    "\n",
    "# Sample data for Sales table\n",
    "sales_data = {\n",
    "    'product_id': [1, 2, 3],\n",
    "    'period_start': ['2019-01-25', '2018-12-01', '2019-12-01'],\n",
    "    'period_end': ['2019-02-28', '2020-01-01', '2020-01-31'],\n",
    "    'average_daily_sales': [100, 10, 1]\n",
    "}\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "sales_data['period_start'] = pd.to_datetime(sales_data['period_start'])\n",
    "sales_data['period_end'] = pd.to_datetime(sales_data['period_end'])\n",
    "\n",
    "# Create DataFrame for Product and Sales tables\n",
    "product_df = pd.DataFrame(product_data)\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "\n",
    "df_person = spark.createDataFrame(product_df)\n",
    "df_person.createOrReplaceTempView(\"Product\")\n",
    "\n",
    "df_person = spark.createDataFrame(sales_df)\n",
    "df_person.createOrReplaceTempView(\"Sales\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id product_name\n",
      "0           1     LC Phone\n",
      "1           2   LC T-Shirt\n",
      "2           3  LC Keychain\n",
      "\n",
      "   product_id period_start period_end  average_daily_sales\n",
      "0           1   2019-01-25 2019-02-28                  100\n",
      "1           2   2018-12-01 2020-01-01                   10\n",
      "2           3   2019-12-01 2020-01-31                    1\n"
     ]
    }
   ],
   "source": [
    "print(product_df)\n",
    "print()\n",
    "print(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select\n",
    "        *,\n",
    "        explode(sequence(year(period_start), year(period_end))) as year_gen\n",
    "    from Sales s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+--------+\n",
      "|product_id|       period_start|         period_end|average_daily_sales|year_gen|\n",
      "+----------+-------------------+-------------------+-------------------+--------+\n",
      "|         1|2019-01-25 00:00:00|2019-02-28 00:00:00|                100|    2019|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2018|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2019|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2020|\n",
      "|         3|2019-12-01 00:00:00|2020-01-31 00:00:00|                  1|    2019|\n",
      "|         3|2019-12-01 00:00:00|2020-01-31 00:00:00|                  1|    2020|\n",
      "+----------+-------------------+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = f\"\"\"\n",
    "    select\n",
    "        *,\n",
    "        max(year_gen) over(partition by product_id) = year_gen as is_max,\n",
    "        min(year_gen) over(partition by product_id) = year_gen as is_min\n",
    "    from (\n",
    "        {query}\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+--------+------+------+\n",
      "|product_id|       period_start|         period_end|average_daily_sales|year_gen|is_max|is_min|\n",
      "+----------+-------------------+-------------------+-------------------+--------+------+------+\n",
      "|         1|2019-01-25 00:00:00|2019-02-28 00:00:00|                100|    2019|  true|  true|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2018| false|  true|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2019| false| false|\n",
      "|         2|2018-12-01 00:00:00|2020-01-01 00:00:00|                 10|    2020|  true| false|\n",
      "|         3|2019-12-01 00:00:00|2020-01-31 00:00:00|                  1|    2019| false|  true|\n",
      "|         3|2019-12-01 00:00:00|2020-01-31 00:00:00|                  1|    2020|  true| false|\n",
      "+----------+-------------------+-------------------+-------------------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = f\"\"\"\n",
    "    select\n",
    "        t.product_id,\n",
    "        p.product_name,\n",
    "        t.year_gen as year_report,\n",
    "        case\n",
    "            when (is_max and is_min) then average_daily_sales * (\n",
    "                date_diff(\n",
    "                    period_end, \n",
    "                    period_start\n",
    "                ) + 1\n",
    "            )\n",
    "            \n",
    "            when (is_min) then average_daily_sales * date_diff(\n",
    "                to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), \n",
    "                period_start\n",
    "            )\n",
    "\n",
    "            when (is_max) then average_daily_sales * (\n",
    "                date_diff(\n",
    "                    period_end, \n",
    "                    to_date(concat(year_gen, '-01-01'), 'yyyy-MM-dd')\n",
    "                ) + 1\n",
    "            )\n",
    "            \n",
    "            else average_daily_sales * date_diff(\n",
    "                to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), \n",
    "                to_date(concat(year_gen, '-01-01'), 'yyyy-MM-dd')\n",
    "            )\n",
    "            \n",
    "        end as total_amount\n",
    "    from (\n",
    "        {query_1}\n",
    "    ) t left join Product p on t.product_id = p.product_id\n",
    "    order by t.product_id asc, year_report asc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select\n",
      "        t.product_id,\n",
      "        p.product_name,\n",
      "        t.year_gen as year_report,\n",
      "        case\n",
      "            when (is_max and is_min) then average_daily_sales * (\n",
      "                date_diff(\n",
      "                    period_end, \n",
      "                    period_start\n",
      "                ) + 1\n",
      "            )\n",
      "            \n",
      "            when (is_min) then average_daily_sales * date_diff(\n",
      "                to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), \n",
      "                period_start\n",
      "            )\n",
      "\n",
      "            when (is_max) then average_daily_sales * (\n",
      "                date_diff(\n",
      "                    period_end, \n",
      "                    to_date(concat(year_gen, '-01-01'), 'yyyy-MM-dd')\n",
      "                ) + 1\n",
      "            )\n",
      "            \n",
      "            else average_daily_sales * date_diff(\n",
      "                to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), \n",
      "                to_date(concat(year_gen, '-01-01'), 'yyyy-MM-dd')\n",
      "            )\n",
      "            \n",
      "        end as total_amount\n",
      "    from (\n",
      "        \n",
      "    select\n",
      "        *,\n",
      "        max(year_gen) over(partition by product_id) = year_gen as is_max,\n",
      "        min(year_gen) over(partition by product_id) = year_gen as is_min\n",
      "    from (\n",
      "        \n",
      "    select\n",
      "        *,\n",
      "        explode(sequence(year(period_start), year(period_end))) as year_gen\n",
      "    from Sales s\n",
      "\n",
      "    )\n",
      "\n",
      "    ) t left join Product p on t.product_id = p.product_id\n",
      "    order by t.product_id asc, year_report asc\n",
      "\n",
      "+----------+------------+-----------+------------+\n",
      "|product_id|product_name|year_report|total_amount|\n",
      "+----------+------------+-----------+------------+\n",
      "|         1|    LC Phone|       2019|        3500|\n",
      "|         2|  LC T-Shirt|       2018|         310|\n",
      "|         2|  LC T-Shirt|       2019|        3650|\n",
      "|         2|  LC T-Shirt|       2020|          10|\n",
      "|         3| LC Keychain|       2019|          31|\n",
      "|         3| LC Keychain|       2020|          31|\n",
      "+----------+------------+-----------+------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['t.product_id ASC NULLS FIRST, 'year_report ASC NULLS FIRST], true\n",
      "+- 'Project ['t.product_id, 'p.product_name, 't.year_gen AS year_report#1764, CASE WHEN ('is_max AND 'is_min) THEN ('average_daily_sales * ('date_diff('period_end, 'period_start) + 1)) WHEN 'is_min THEN ('average_daily_sales * 'date_diff('to_date('concat(('year_gen + 1), -01-01), yyyy-MM-dd), 'period_start)) WHEN 'is_max THEN ('average_daily_sales * ('date_diff('period_end, 'to_date('concat('year_gen, -01-01), yyyy-MM-dd)) + 1)) ELSE ('average_daily_sales * 'date_diff('to_date('concat(('year_gen + 1), -01-01), yyyy-MM-dd), 'to_date('concat('year_gen, -01-01), yyyy-MM-dd))) END AS total_amount#1765]\n",
      "   +- 'Join LeftOuter, ('t.product_id = 'p.product_id)\n",
      "      :- 'SubqueryAlias t\n",
      "      :  +- 'Project [*, ('max('year_gen) windowspecdefinition('product_id, unspecifiedframe$()) = 'year_gen) AS is_max#1762, ('min('year_gen) windowspecdefinition('product_id, unspecifiedframe$()) = 'year_gen) AS is_min#1763]\n",
      "      :     +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      :        +- 'Project [*, 'explode('sequence('year('period_start), 'year('period_end))) AS year_gen#1761]\n",
      "      :           +- 'SubqueryAlias s\n",
      "      :              +- 'UnresolvedRelation [Sales], [], false\n",
      "      +- 'SubqueryAlias p\n",
      "         +- 'UnresolvedRelation [Product], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: bigint, product_name: string, year_report: int, total_amount: bigint\n",
      "Sort [product_id#4L ASC NULLS FIRST, year_report#1764 ASC NULLS FIRST], true\n",
      "+- Project [product_id#4L, product_name#1, year_gen#1766 AS year_report#1764, CASE WHEN (is_max#1762 AND is_min#1763) THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), cast(period_start#5 as date)) + 1) as bigint)) WHEN is_min#1763 THEN (average_daily_sales#7L * cast(date_diff(to_date(concat(cast((year_gen#1766 + 1) as string), -01-01), Some(yyyy-MM-dd), Some(Etc/UTC), false), cast(period_start#5 as date)) as bigint)) WHEN is_max#1762 THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), to_date(concat(cast(year_gen#1766 as string), -01-01), Some(yyyy-MM-dd), Some(Etc/UTC), false)) + 1) as bigint)) ELSE (average_daily_sales#7L * cast(date_diff(to_date(concat(cast((year_gen#1766 + 1) as string), -01-01), Some(yyyy-MM-dd), Some(Etc/UTC), false), to_date(concat(cast(year_gen#1766 as string), -01-01), Some(yyyy-MM-dd), Some(Etc/UTC), false)) as bigint)) END AS total_amount#1765L]\n",
      "   +- Join LeftOuter, (product_id#4L = product_id#0L)\n",
      "      :- SubqueryAlias t\n",
      "      :  +- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766, is_max#1762, is_min#1763]\n",
      "      :     +- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766, _we0#1769, _we1#1770, (_we0#1769 = year_gen#1766) AS is_max#1762, (_we1#1770 = year_gen#1766) AS is_min#1763]\n",
      "      :        +- Window [max(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1769, min(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1770], [product_id#4L]\n",
      "      :           +- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766]\n",
      "      :              +- SubqueryAlias __auto_generated_subquery_name\n",
      "      :                 +- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766]\n",
      "      :                    +- Generate explode(sequence(year(cast(period_start#5 as date)), year(cast(period_end#6 as date)), None, Some(Etc/UTC))), false, [year_gen#1766]\n",
      "      :                       +- SubqueryAlias s\n",
      "      :                          +- SubqueryAlias sales\n",
      "      :                             +- View (`Sales`, [product_id#4L,period_start#5,period_end#6,average_daily_sales#7L])\n",
      "      :                                +- LogicalRDD [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L], false\n",
      "      +- SubqueryAlias p\n",
      "         +- SubqueryAlias product\n",
      "            +- View (`Product`, [product_id#0L,product_name#1])\n",
      "               +- LogicalRDD [product_id#0L, product_name#1], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [product_id#4L ASC NULLS FIRST, year_report#1764 ASC NULLS FIRST], true\n",
      "+- Project [product_id#4L, product_name#1, year_gen#1766 AS year_report#1764, CASE WHEN (is_max#1762 AND is_min#1763) THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), cast(period_start#5 as date)) + 1) as bigint)) WHEN is_min#1763 THEN (average_daily_sales#7L * cast(date_diff(cast(gettimestamp(concat(cast((year_gen#1766 + 1) as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date), cast(period_start#5 as date)) as bigint)) WHEN is_max#1762 THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), cast(gettimestamp(concat(cast(year_gen#1766 as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)) + 1) as bigint)) ELSE (average_daily_sales#7L * cast(date_diff(cast(gettimestamp(concat(cast((year_gen#1766 + 1) as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date), cast(gettimestamp(concat(cast(year_gen#1766 as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)) as bigint)) END AS total_amount#1765L]\n",
      "   +- Join LeftOuter, (product_id#4L = product_id#0L)\n",
      "      :- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766, (_we0#1769 = year_gen#1766) AS is_max#1762, (_we1#1770 = year_gen#1766) AS is_min#1763]\n",
      "      :  +- Window [max(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1769, min(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1770], [product_id#4L]\n",
      "      :     +- Generate explode(sequence(year(cast(period_start#5 as date)), year(cast(period_end#6 as date)), None, Some(Etc/UTC))), false, [year_gen#1766]\n",
      "      :        +- LogicalRDD [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L], false\n",
      "      +- Filter isnotnull(product_id#0L)\n",
      "         +- LogicalRDD [product_id#0L, product_name#1], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [product_id#4L ASC NULLS FIRST, year_report#1764 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(product_id#4L ASC NULLS FIRST, year_report#1764 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2278]\n",
      "      +- Project [product_id#4L, product_name#1, year_gen#1766 AS year_report#1764, CASE WHEN (is_max#1762 AND is_min#1763) THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), cast(period_start#5 as date)) + 1) as bigint)) WHEN is_min#1763 THEN (average_daily_sales#7L * cast(date_diff(cast(gettimestamp(concat(cast((year_gen#1766 + 1) as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date), cast(period_start#5 as date)) as bigint)) WHEN is_max#1762 THEN (average_daily_sales#7L * cast((date_diff(cast(period_end#6 as date), cast(gettimestamp(concat(cast(year_gen#1766 as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)) + 1) as bigint)) ELSE (average_daily_sales#7L * cast(date_diff(cast(gettimestamp(concat(cast((year_gen#1766 + 1) as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date), cast(gettimestamp(concat(cast(year_gen#1766 as string), -01-01), yyyy-MM-dd, TimestampType, Some(Etc/UTC), false) as date)) as bigint)) END AS total_amount#1765L]\n",
      "         +- SortMergeJoin [product_id#4L], [product_id#0L], LeftOuter\n",
      "            :- Project [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L, year_gen#1766, (_we0#1769 = year_gen#1766) AS is_max#1762, (_we1#1770 = year_gen#1766) AS is_min#1763]\n",
      "            :  +- Window [max(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1769, min(year_gen#1766) windowspecdefinition(product_id#4L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1770], [product_id#4L]\n",
      "            :     +- Sort [product_id#4L ASC NULLS FIRST], false, 0\n",
      "            :        +- Exchange hashpartitioning(product_id#4L, 200), ENSURE_REQUIREMENTS, [plan_id=2267]\n",
      "            :           +- Generate explode(sequence(year(cast(period_start#5 as date)), year(cast(period_end#6 as date)), None, Some(Etc/UTC))), [product_id#4L, period_start#5, period_end#6, average_daily_sales#7L], false, [year_gen#1766]\n",
      "            :              +- Scan ExistingRDD[product_id#4L,period_start#5,period_end#6,average_daily_sales#7L]\n",
      "            +- Sort [product_id#0L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(product_id#0L, 200), ENSURE_REQUIREMENTS, [plan_id=2273]\n",
      "                  +- Filter isnotnull(product_id#0L)\n",
      "                     +- Scan ExistingRDD[product_id#0L,product_name#1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(query_2)\n",
    "spark.sql(query_2).show()\n",
    "spark.sql(query_2).explain(extended=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select\n",
      "        *,\n",
      "        case\n",
      "            when is_max and is_min then average_daily_sales * (date_diff(period_end, period_start) + 1)\n",
      "            when is_min then average_daily_sales * (date_diff(to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), period_start))\n",
      "            when is_max then average_daily_sales * (date_diff(to_date(concat(year_gen + 1, '-01-01'), 'yyyy-MM-dd'), period_start))\n",
      "        end as value\n",
      "    from (\n",
      "        \n",
      "    select\n",
      "        *,\n",
      "        max(year_gen) over(partition by product_id) = year_gen as is_max,\n",
      "        min(year_gen) over(partition by product_id) = year_gen as is_min\n",
      "    from (\n",
      "        \n",
      "    select\n",
      "        *,\n",
      "        Year(period_end) - Year(period_start) + 1 as diff_year,\n",
      "        date_diff(period_end, period_start) as diff_date,\n",
      "        date_add(period_end, 1) as date_end_cal,\n",
      "        Year(period_end) - Year(period_start) as date_gen,\n",
      "        explode(sequence(year(period_start), year(period_end))) as year_gen,\n",
      "        average_daily_sales * 365 as total_each_year\n",
      "    from Sales s\n",
      "\n",
      "    )\n",
      "\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(query_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
